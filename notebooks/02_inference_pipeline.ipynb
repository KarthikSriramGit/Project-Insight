{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Configure runtime first:** Runtime > Change runtime type > Hardware accelerator: **GPU** (T4) > Save."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference Pipeline and Format Selection (Course 2)\n",
        "\n",
        "This notebook demonstrates model format selection and the inference pipeline. Inspired by [Intro to Inference: How to Run AI Models on a GPU](https://developers.google.com/learn/pathways/ai-models-on-gpu-intro).\n",
        "\n",
        "Run the setup cell first. Use **Runtime > Change runtime type > GPU** in Colab for faster inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Colab setup: clone repo and install dependencies (run this cell first)\n",
        "try:\n",
        "    import google.colab\n",
        "    get_ipython().system(\"git clone -q https://github.com/KarthikSriramGit/Project-Insight.git\")\n",
        "    get_ipython().run_line_magic(\"cd\", \"Project-Insight\")\n",
        "    get_ipython().system(\"pip install -q -r requirements.txt\")\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: Colab runs from repo root after clone\n",
        "import sys\n",
        "from pathlib import Path\n",
        "ROOT = Path(\".\").resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "print(f\"ROOT={ROOT}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROOT=C:\\Users\\skart\\Desktop\\ROG SSD\\Git Repos\\Project-Insight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Format selection by use case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.inference.format_selector import select_format\n",
        "\n",
        "for use_case in [\"research\", \"sharing\", \"local\", \"production\", \"portable\"]:\n",
        "    fmt, rationale = select_format(use_case, hardware=\"gpu\")\n",
        "    print(f\"{use_case}: {fmt}\")\n",
        "    print(f\"  {rationale[:85]}...\")\n",
        "    print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "research: safetensors\n",
            "  Fast, secure weight serialization. Memory-mapped loading, no arbitrary code execution...\n",
            "\n",
            "sharing: safetensors\n",
            "  Fast, secure weight serialization. Memory-mapped loading, no arbitrary code execution...\n",
            "\n",
            "local: gguf\n",
            "  Compact, quantized format for local inference. Powers llama.cpp and run-on-laptop wor...\n",
            "\n",
            "production: tensorrt\n",
            "  Compiled engine for NVIDIA GPUs. Pre-optimized kernels, lowest latency and highest th...\n",
            "\n",
            "portable: onnx\n",
            "  Graph-level interchange format. Framework-agnostic, runs on ONNX Runtime, OpenVINO, T...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inference metrics (p50, p90, throughput)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.inference.metrics import compute_metrics\n",
        "\n",
        "total_latencies = [1.2, 1.1, 1.3, 1.0, 1.2]\n",
        "first_token_latencies = [0.1, 0.09, 0.11, 0.1, 0.1]\n",
        "token_counts = [64, 64, 64, 64, 64]\n",
        "\n",
        "metrics = compute_metrics(\n",
        "    total_latencies=total_latencies,\n",
        "    first_token_latencies=first_token_latencies,\n",
        "    token_counts=token_counts,\n",
        ")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p50_latency_s: 1.2000\n",
            "p90_latency_s: 1.2600\n",
            "p50_ttft_s: 0.1000\n",
            "p90_ttft_s: 0.1060\n",
            "throughput_sustained_tok_s: 55.1724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inference pipeline with TinyLlama (no Hugging Face login)\n",
        "\n",
        "Uses TinyLlama 1.1B, a public model. For Gemma, add `from huggingface_hub import login; login()` first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dependencies installed by setup cell; enable GPU in Runtime > Change runtime type"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference pipeline with TinyLlama (requires GPU runtime for best performance)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from src.inference.pipeline import InferencePipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "pipe = InferencePipeline(model, tokenizer, device=device, max_new_tokens=64)\n",
        "out = pipe.generate([\"What was the peak brake pressure in vehicle V001?\"], max_new_tokens=32)\n",
        "print(out[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 201/201 [00:03<00:00, 51.98it/s, Materializing param=model.norm.weight]                              \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "What was the peak brake pressure in vehicle V001? \n",
            "<|assistant|>\n",
            "The peak brake pressure in vehicle V001 was 120 psi (8.2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}