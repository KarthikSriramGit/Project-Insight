{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline and Format Selection (Course 2)\n",
    "\n",
    "This notebook demonstrates model format selection and the inference pipeline (tokenization, batching, forward pass, decoding). Inspired by [Intro to Inference: How to Run AI Models on a GPU](https://developers.google.com/learn/pathways/ai-models-on-gpu-intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format selection by use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from src.inference.format_selector import select_format, FORMAT_RATIONALE\n",
    "\n",
    "for use_case in [\"research\", \"sharing\", \"local\", \"production\", \"portable\"]:\n",
    "    fmt, rationale = select_format(use_case, hardware=\"gpu\")\n",
    "    print(f\"{use_case}: {fmt}\")\n",
    "    print(f\"  {rationale[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference metrics (p50, p90, throughput)\n",
    "\n",
    "Metrics: total latency, TTFT, sustained throughput, inter-token latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.metrics import compute_metrics, timed_generate\n",
    "\n",
    "total_latencies = [1.2, 1.1, 1.3, 1.0, 1.2]\n",
    "first_token_latencies = [0.1, 0.09, 0.11, 0.1, 0.1]\n",
    "token_counts = [64, 64, 64, 64, 64]\n",
    "\n",
    "metrics = compute_metrics(\n",
    "    total_latencies=total_latencies,\n",
    "    first_token_latencies=first_token_latencies,\n",
    "    token_counts=token_counts,\n",
    ")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline with telemetry query\n",
    "\n",
    "To run the full pipeline with a real model, load a small LLM (e.g. Gemma 270M) and use InferencePipeline. Preprocessing, batching, forward pass, and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: pipeline usage (requires transformers, torch, and HF token)\n",
    "# from src.inference.pipeline import InferencePipeline\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m-it\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m-it\", device_map=\"cuda\")\n",
    "# pipe = InferencePipeline(model, tokenizer, device=\"cuda\")\n",
    "# out = pipe.generate([\"What was the peak brake pressure in vehicle V001?\"], max_new_tokens=64)\n",
    "# print(out[0])\n",
    "print(\"Pipeline usage: see comments above. Requires transformers, torch, GPU.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
